# -*- coding: utf-8 -*-
"""221111.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EbO-uPLty8luu5O327hYMY3ld1Lamwm6

질문
"""

# book = [103, 205, 405]

# 어제 예약한 목록
# book.append(107)
# book.append(203)
# book.append(304)
# book.append(507)
# print (book)

# # 오늘 예약한 목록
# book.remove(205)
# book.remove(304)
# print (book)

book = [103, 205, 405]
book2 = [107, 203, 304, 507]
book = book + book2

# 205, 304 지우는 방법
# 1. remove
# 2. sort해서 슬라이싱, 지우기. 이걸 왜 알려주는 거여
book.sort()
print (book)
# del (book [4:6])
# print (book)

# 3. 
# book.index(205)
# del (book[book.index(205)])
# del (book[book.index(304)])
# print (book)

# 4. list comprehension 도대체 이걸 왜 for문으로 돌리는 거여.............. 하면야 할 수 있겠지만....
book3 = [i for i in book if i != 205 and i != 304]
print (book3)

"""어제꺼~~~~"""



import pandas as pd
import numpy as np
print (pd.__version__) # pandas 버전 확인

"""- DataFrame은 2차원 테이블이고 테이블의 한 줄(행 or 열)을 시리즈라고 합니다"""

s = pd.Series([1,3,5, np.nan, 6, 8])
s

# 2021 01 01부터 6일간의 널짜 범위를 생성
dates = pd.date_range('20210101', periods=6)
dates

# 6*4 행렬에 -1에서 1 사이의 랜덤한 숫자를 가지는 원소를 넣고
# index열은 dates, 컬럼은 순서대로 abcd로 하는 DataFrame
df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=['A', 'B', 'C', 'D'])
df

"""### 5.3 DataFrame 기초 method"""

#맨 위 다섯줄을 보여주는 head()
df.head()

# 원하는 줄수만큼 볼 수도 있음
df.head(3)

# 끝부분도 볼 수 있음
df.tail(2)

df.index

df.columns

df.values

df.info()

df.describe()
# 간단한 통계값
# 평균
# 표준편차
# 최소값
# 상위25%
# 중위값
# 상위75%
# 최대값

df.sort_values(by='A', ascending=False).head(3)
# A 기준으로 상위 3개값인데, 정렬은 내림차순 (ascending:  ##내림차순)

"""### 5.4 dataFrame indexing"""

df['A']
# 컬럼으로 가져올 수 있다. 파이썬은 컬럼을 인덱스로 생각함
# df['2021-01-01'] 이런거 에러남

# 특정 날짜를 통한 인덱싱
df.loc['2021-01-01']

# 슬라이싱을 이용하면 row 단위로 잘라짐
# 위에서부터 3줄을 잘라옴
df[:3]

# 안좋은 코드래
# df에 이미 dates가 있는데 다시 dates를 볼 필요 없으니까
df.loc[dates[0]]

# 지금은 a, c 두개지만 많아지면..? 
df.loc[:,['A','C']]

# bad
df.loc['2021-01-02',['A','B']]

df.loc['2021-01-01', 'C']

# 안좋은 코드 끝?!

df['A'] >0

df.loc[:,'A'] >0

df[df['A']>0]

df[df['A']>0]['B']

# 데이터프레임 안에 있는 값을 바꾸는 것도 가능
df[df<0] = 0
df

"""~~~~~~ 어제꺼"""

df2 = df.copy()

df2['E'] = ['one', 'two', 'three', 'four', 'five', 'six']

df2['E'].isin(['two', 'four'])

#API 통신을 할때 사용하는 라이브러리
import requests
#가져온 페이지의 html을 파싱하는 라이브러리
from bs4 import BeautifulSoup

url = "https://comic.naver.com/webtoon/weekday"
#지정한 url로 페이지를 가져오는 요청을 생성함
res = requests.get(url)
res.raise_for_status()
#요청을 통해 가져온 페이지를 BeautifulSoup으로 파싱함
#lxml : html 및 xml의 파싱을 간편하게 도와주는 라이브러리
soup = BeautifulSoup(res.text,"lxml")

print(type(soup))

print(soup.head)

# html 문서의 바디 태그에 해당하는 내용 출력
print ('---------------------------soup.body---------------------------')
print (soup.body)

# title 태그에 해당하는 내용
print ('---------------------------soup.title---------------------------')
print (soup.title)

# title 태그명에 해당하는 내용
print ('---------------------------soup.title.name---------------------------')
print (soup.title.name)

# title 태그명에 해당하는 내용
print ('---------------------------soup.title.string---------------------------')
print (soup.title.string)

# 이 태그 중에 클래스 명이 asideBoxRank인 html 요소들을 가져옴
webtoonBox = soup.find('ol', attrs = {'class': 'asideBoxRank'})
print ('---------------webtoonBox-------------------')
print (webtoonBox)

# 추출한 html 요소들 중 a태그에 해당되는 내용을 전부 가져옴
webtoons = webtoonBox.find_all('a')
print ('---------------webtoons-------------------')
print (webtoons)

for test in enumerate(webtoons):
  print (test)

# a 태그를 가져온 리스트 중 title만 뽑아와서 새로 리스트화 방법1
webtoonRankList1 = [(str(idx), webtoon.get('title')) for (idx, webtoon) in enumerate(webtoons, start=1)]
print (webtoonRankList1)

# a태그를 가져온 리스트 중 title만 뽑아와서 새로 리스트화 방법2
webtoonRankList = []
for (idx, webtoon) in enumerate(webtoons, start=1):
  title = webtoon.get("title")
  print (f'{str(idx)} : {title}')
  data = [str(idx), title]
  webtoonRankList.append(data)
print (webtoonRankList)

import pandas as pd
# 데이터프레임화
pd.DataFrame(webtoonRankList, columns=['ranks', 'title'])
pd.DataFrame(webtoonRankList1, columns=['ranks', 'title'])

"""요일별 전체 웹툰 크롤링"""

url2 = 'https://comic.naver.com/webtoon/weekday'
# 지정한 url로 페이지를 가져오는 요청 생성
res2 = requests.get(url2)
res2.raise_for_status()
# 요청을 통해 가져온 페이지를 BeatifulSoup로 파싱함

soup2 = BeautifulSoup(res2.text,'lxml')

# div 태그중에 class명이 list_area daily_all인 html 요소들 가져옴
webtoonByDayClass = soup2.find('div', attrs = {'class' : 'list_area daily_all'})
print ('-----------------------------------------webtoonByDay--------')
print (webtoonByDayClass)
# 가져ㅏ온 html 요소들 중 img 태그를 가져옴
webtoonByDayImg = webtoonByDayClass.find_all('img')
print ('----------------------webtoonByDay---------------------')
print (webtoonByDayImg)

# a 태그를 가져온 리스트 중 title만 뽑아와서 새로 리스트화
webtoonByDayList = []
for (idx, webtoon) in enumerate(webtoonByDayImg, start=1):
  title = webtoon.get('title')
  # 혹시 title이 없는 경우 실행하지 않기 위함
  if title == None:
    continue
  print(f"#{str(idx)}: {title}")
  data = [str(idx), title]
  webtoonByDayList.append(data)

pd.DataFrame(webtoonByDayList, columns = ['idx', 'title'])

# 요일까지 가져오기 위해서는 다른 col_inner라는 태그 안의 h4 태그의 text를 가져와야함
webtoonByDayCol = webtoonByDayClass.find_all ('div', attrs = {'class': 'col_inner'})
print (webtoonByDayCol)

webtoonByDayListWithImg = []
i = 1
for (idx, webtoon) in enumerate (webtoonByDayCol, start=1):
  # 가져온 html 태그 중 h4 태그의 text를 가져옴
  h4 = webtoon.find('h4').text
  #가져온 html 태그중 img  태그를 모드 가져와서 리스트화
  imgs = webtoon.find_all('img')
  print (h4)
  print(imgs)
  # 리스트화된 img 태그들을 반복문을 통해 정제
  for img in imgs:
    title = img.get('title')
    # img 태그 중 src 요소에 썸네일 주소가 담겨있음
    thumbnail = img.get('src')
    if title == None:
      continue
    print (f'{str(i)}: {h4}, {title}, {thumbnail}')
    data = [str(i), h4, title, thumbnail]
    i += 1
    webtoonByDayListWithImg.append(data)

import os
import sys
import urllib.request
import json

client_id = "P8QYnfhqgLDZ5s4rsoLe"
client_secret = "4RnkINLXw4"

encText = urllib.parse.quote("검색할 단어")
url = "https://openapi.naver.com/v1/search/blog?query=" + encText # JSON 결과
# url = "https://openapi.naver.com/v1/search/blog.xml?query=" + encText # XML 결과
request = urllib.request.Request(url)
request.add_header("X-Naver-Client-Id",client_id)
request.add_header("X-Naver-Client-Secret",client_secret)
response = urllib.request.urlopen(request)
rescode = response.getcode()
if(rescode==200):
    response_body = response.read()
    print(response_body.decode('utf-8'))
else:
    print("Error Code:" + rescode)

# 검색키워드 
searchText = input ('검새기워드: ')
query = urllib.parse.quote(searchText)
# 출력건수
display = input ('검색개수: ')
# url에 요청 변ㅅ들 중 필수 변수는 반드시 넣어주어야 하며 이외의 변수는 선택
url = 'https://openapi.naver.com/v1/search/blog'
query = f'?query={query}&display = {display} & start=1 & sort=sim'

# urll과 쿼리를 연결해 요청을 생성 
request = urllib.request.Request(url+query)
# 네이버 api를 활용하기 위한 키값들을 요처ㅓㅇ 헤더에 추가